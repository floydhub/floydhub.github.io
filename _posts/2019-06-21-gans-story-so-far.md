---
author: Ajay Uppili Arasanipalai
date: 2019-06-21 10:17:08 +0000
excerpt: Generative adversarial networks (GANs) have been the go-to state of the art
  algorithm to image generation in the last few years. In this article, you will learn
  about the most significant breakthroughs in this field, including BigGAN, StyleGAN,
  and many more.
feature_image: /assets/images/hero/gans-story-so-far-hero.jpg
layout: post
slug: gans-story-so-far
tags: '[]'
title: Generative Adversarial Networks - The Story So Far
---

When Ian Goodfellow dreamt up the idea of Generative Adversarial Networks (GANs) over a mug of beer back in 2014, he probably didn‚Äôt expect to see the field advance so fast:

![Source](https://regmedia.co.uk/2018/10/01/biggan.jpg)[Source](https://regmedia.co.uk/2018/10/01/biggan.jpg)

In case you don‚Äôt see where I‚Äôm going here, the images you just saw were utterly, undeniably, 100% ‚Ä¶ fake.

Also, I don‚Äôt mean these were photoshopped, CGI-ed, or (fill in the blanks with whatever Nvidia‚Äôs calling their fancy new tech at the moment).

I mean that these images are entirely generated through addition, multiplication, and splurging ludicrous amounts of cash on GPU computation.

The algorithm that makes is stuff work is called a generative adversarial network (which is the long way of writing GAN, for those of you still stuck in machine learning acronym land), and over the last few years, there have been more innovations dedicated to making it work than there have been privacy scandals at Facebook.

> 4.5 years of GAN progress on face generation. <https://t.co/kiQkuYULMC> <https://t.co/S4aBsU536b> <https://t.co/8di6K6BxVC> <https://t.co/UEFhewds2M> <https://t.co/s6hKQz9gLz> [pic.twitter.com/F9Dkcfrq8l](https://t.co/F9Dkcfrq8l)
> 
> -- Ian Goodfellow (@goodfellow_ian) [January 15, 2019](https://twitter.com/goodfellow_ian/status/1084973596236144640?ref_src=twsrc%5Etfw)

Summarizing every single improvement to the 2014 vanilla GANs is about as hard as watching season 8 of Game of Thrones on repeat. So instead, I‚Äôm going to recap the key ideas behind some of the coolest results in GAN research over the years.

I‚Äôm not going to explain concepts like transposed convolutions and Wasserstein distance in detail. Instead, I‚Äôll provide links to some of the best resources you can use to quickly learn about these concepts so that you can see how they fit into the big picture.

If you‚Äôre still reading, I‚Äôm going to assume that you know the basics of deep learning and that you know [how convolutional neural networks work](https://floydhub.github.io/building-your-first-convnet/).

With that said, here‚Äôs the map of the GAN landscape:

![Ha! You thought I was joking when I said map, didn‚Äôt you.](https://paper-attachments.dropbox.com/s_D85DDA7D01FD04AEE96825C4B90F1126BC7D080CA4F2947D4A5DEC07FAD6122C_1559841150851_Map.png)GAN Roadmap

We‚Äôll traverse it one step at a time. Let the journey begin.

  1. GAN: Generative Adversarial Networks
  2. DCGAN: Deep Convolutional Generative Adversarial Network
  3. CGAN: Conditional Generative Adversarial Network
  4. CycleGAN
  5. CoGAN: Coupled Generative Adversarial Networks
  6. ProGAN: Progressive growing of Generative Adversarial Networks
  7. WGAN: Wasserstein Generative Adversarial Networks
  8. SAGAN: Self-Attention Generative Adversarial Networks
  9. BigGAN: Big Generative Adversarial Networks
  10. StyleGAN: Style-based Generative Adversarial Networks

### Ready to build, train, and deploy AI?

#### Get started with FloydHub's collaborative AI platform for free

###### [Try FloydHub for free ](https://www.floydhub.com/?utm_source=blog&utm_medium=banner-gans&utm_campaign=try_floydhub_for_free)

## GAN: Generative Adversarial Networks

![](/assets/images/content/images/2019/06/image.png)Image from [paper](https://arxiv.org/pdf/1406.2661.pdf)

  * [Paper](https://arxiv.org/abs/1406.2661)
  * [Code](https://github.com/goodfeli/adversarial)
  * Other great resources: [Ian Goodfellow‚Äôs NIPS 2016 tutorial](https://arxiv.org/abs/1701.00160)

Now, I know what you‚Äôre thinking ‚Äî geez, that creepy, pixelated mess of an image looks like it was made by a math nerd zooming out of an excel sheet.

Well, guess what, you‚Äôre somewhat right (minus the excel part).

Way back in 2014, Ian Goodfellow proposed a revolutionary idea ‚Äî make two neural networks compete (or collaborate, it‚Äôs a matter of perspective) with each other.

One neural network tries to **generate** realistic data (note that GANs can be used to model any data distribution, but are mainly used for images these days), and the other network tries to **discriminate** between real data and data generated by the generator network.

The generator network uses the discriminator as a loss function and updates its parameters to generate data that starts to look more realistic.

![Source](https://skymind.ai/images/wiki/GANs.png)[Source](https://skymind.ai/images/wiki/GANs.png)

The discriminator network, on the other hand, updates its parameters to make itself better at picking out fake data from real data. So it too gets better at its job.

The game of cat and mouse continues, until the system reaches a so-called ‚Äúequilibrium,‚Äù where the generator creates data that looks real enough that the best the discriminator can do is guess randomly.

Hopefully, by this point, if you indented your code correctly and Amazon decided not to kill your spot instances(by the way, this will not happen on FloydHub since they provide reserved GPU machines), you're now left with a generator that accurately creates new data from the same distribution as your training set.

Now, this is admittedly a very simplistic view of GANs. The idea you need to take away from here is that by using two neural networks - one to generate data and one to classify real data from fake data, you can simultaneously train them to, in theory, converge to a point where the generator can generate completely new, realistic data.

## DCGAN: Deep Convolutional Generative Adversarial Network

![](/assets/images/content/images/2019/06/image-1.png)Image from [paper](https://arxiv.org/pdf/1511.06434.pdf)

  * [Paper](https://arxiv.org/abs/1511.06434)
  * [Code](https://github.com/floydhub/dcgan)
  * Other great resources: [Medium article](https://towardsdatascience.com/up-sampling-with-transposed-convolution-9ae4f2df52d0)

Look, I‚Äôll save you some time.

$$\text{Convolutions} = \text{Good for images}$$  
$$\text{GANs} = \text{Good for generating stuff}$$  
$$\implies \text{Convolutions} + \text{GANs} = \text{Good for generating images}$$

In hindsight, as Ian Goodfellow himself pointed out in a [podcast with Lex Fridman](https://www.youtube.com/watch?v=Z6rxFNMGdn0&t=41m27s), it seems silly to call this model DCGAN (short for ‚Äúdeep convolutional generative adversarial network‚Äù) since almost everything related to deep learning and images these days is deep and convolutional.

Plus, when most people are introduced to GANs, they learn about them in the ‚Äúdeep and convolutional‚Äù setting anyway. ü§∑

Nevertheless, there existed a time when GANs did not necessarily use convolution-based operation and instead relied upon a standard multi-layer perceptron architecture.

DCGAN changed that by using something called a transposed convolution operation or, _its "unfortunate" name,_ __[Deconvolution layer](https://datascience.stackexchange.com/questions/6107/what-are-deconvolutional-layers)_ ,_.

Transposed convolutions act as an upscaling operation. They help us transform low-resolution images into higher resolution ones.

But seriously, go through the above resources to nail down your understanding of transposed convolutions, since this operation is fundamental to all modern GAN architectures.

If you‚Äôre a little short on time though, an excellent summary of how transposed convolutions work can be found in one simple animation:

![Source](https://paper-attachments.dropbox.com/s_D85DDA7D01FD04AEE96825C4B90F1126BC7D080CA4F2947D4A5DEC07FAD6122C_1559895100810_s_257816875AB071372D764C43DFB8A4901E0EBAB81800E986F1F30C17BAA9E957_1557221973576_no_padding_strides_transposed.gif)[Source](https://arxiv.org/abs/1603.07285)

In vanilla convnets, you apply a sequence of convolutions (along with other operations) to map an image to a vector that usually lower dimensional.

Similarly, applying multiple transposed convolutions sequentially allows us to evolve a single, low-resolution array into a vibrant, full-color image.

* * *

Now before we continue, let‚Äôs venture off on a small tangent to explore some unique ways of using GANs.

![](https://paper-attachments.dropbox.com/s_D85DDA7D01FD04AEE96825C4B90F1126BC7D080CA4F2947D4A5DEC07FAD6122C_1559921735372_Map2.png)You're currently at the second red 'X'

## CGAN: Conditional Generative Adversarial Network

![Source](https://paper-attachments.dropbox.com/s_D85DDA7D01FD04AEE96825C4B90F1126BC7D080CA4F2947D4A5DEC07FAD6122C_1559840652472_Screenshot+2019-06-06+at+10.33.58+PM.png)Image from [paper](https://arxiv.org/pdf/1411.1784.pdf)

  * [Paper](https://arxiv.org/abs/1411.1784)
  * [Code](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras)
  * Other great resources: [Blogpost](https://wiseodd.github.io/techblog/2016/12/24/conditional-gan-tensorflow/)

The original GAN generates data from random noise. That means that you can train it on, say, dog images, and it would generate more dog images.

You could also train it on cat images, in which case, it would generate cat images.

You could also train it on Nicholas Cage images, in which case, it would generate Nicholas Cage images.

You could also train it on‚Ä¶ I hope you‚Äôre sensing a pattern here.

However, if you try to train it on both dog and cat images at the same time, it‚Äôll produce blurry half-breeds.

![Photo by Anusha Barwa](https://images.unsplash.com/photo-1509205477838-a534e43a849f?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjI2NzI5fQ)Photo by [Anusha Barwa](https://unsplash.com/@anshaaleena?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)

CGAN (which stands for ‚Äúconditional generative adversarial network‚Äù) aims to solve this issue by telling the generator to generate images of only one particular class, like a cat, dog, or Nicholas Cage.

Specifically, CGAN concatenates a one-hot vector $y$ to the random noise vector $z$ to result in an architecture that looks like this:

![Source](https://paper-attachments.dropbox.com/s_D85DDA7D01FD04AEE96825C4B90F1126BC7D080CA4F2947D4A5DEC07FAD6122C_1559840765144_Screenshot+2019-06-06+at+10.35.29+PM.png)[Source](https://arxiv.org/pdf/1411.1784.pdf)

Now, we can generate both cats **and** dogs from the very same GAN. üêà üêï

## CycleGAN

![Source](https://cdn-images-1.medium.com/max/1600/1*5DG4hHjxAyWTfV1J3mRH_A.png)[Source](https://cdn-images-1.medium.com/max/1600/1*5DG4hHjxAyWTfV1J3mRH_A.png)

  * [Paper](https://arxiv.org/abs/1703.10593v6)
  * [Code](https://github.com/junyanz/CycleGAN)
  * Other great resources: [CycleGAN project page](https://junyanz.github.io/CycleGAN/), [Medium article]( https://towardsdatascience.com/turning-fortnite-into-pubg-with-deep-learning-cyclegan-2f9d339dcdb0)

GANs aren‚Äôt used just for generating images. They can also create horse+zebra creatures (zorses? hobras?) like in the image above.

To create these images, CycleGAN aims to solve a problem called image-to-image translation.

CycleGAN isn‚Äôt a new GAN architecture that pushes the state of the art image synthesis. Instead, it‚Äôs a smart way of using GANs. So you‚Äôre free to use this technique for any architecture you like.

Right off the bat, I‚Äôm going to recommend that you read [this paper](https://arxiv.org/abs/1703.10593v6). It‚Äôs exceptionally well written and is quite accessible, even to beginners.

The task here is to train a network $G(X)$ that maps images from a source domain $X$ to a target domain $Y$.

But wait, ‚Äúhow is this different from regular deep learning or style transfer,‚Äù you ask.

Well, the image below summarizes it well. CycleGAN performs unpaired image to image translation. That means the images we‚Äôre training on don‚Äôt have to represent the same thing.

![Source](https://cdn-images-1.medium.com/max/1200/1*oZsw1JaGkKPxWKKvVUWlyg.png)[Source](https://cdn-images-1.medium.com/max/1200/1*oZsw1JaGkKPxWKKvVUWlyg.png)

It would be (relatively) easy to DaVinci-ify images if we had a large collection of $\text{(image, painting of image by da Vinci)}$ pairs.

Unfortunately, the dude didn‚Äôt get around to completing too many paintings. 

CycleGAN, however, trains on unpaired data. So we don‚Äôt need two images of the same thing.

On the other hand, we could use style transfer. But that would just extract the style of one particular image and transfer it to another image, meaning we can‚Äôt translate from say, horses to zebras.

CycleGAN, however, learns a mapping from one domain of images to another. So we train it on say, the set of all Monet paintings.

![](/assets/images/content/images/2019/06/68747470733a2f2f6a756e79616e7a2e6769746875622e696f2f4379636c6547414e2f696d616765732f70686f746f327061696e74696e672e6a7067.jpeg)[Source](https://camo.githubusercontent.com/1862adecd202ba420847653d9a119f9fed9a3abd/68747470733a2f2f6a756e79616e7a2e6769746875622e696f2f4379636c6547414e2f696d616765732f70686f746f327061696e74696e672e6a7067)

The method they use is quite elegant. CycleGAN consists of two generators, $G$, and $F$, and two discriminators, $D_X$ and $D_Y$.

$G$ takes in an image from $X$ and tries to map it to some image in $Y$. The discriminator $D_Y$ predicts whether an image was generated by $G$ or was actually in $Y$.

Similarly, $F$ takes in an image from $Y$ and tries to map it to some image in $X$, And the discriminator $D_X$ predicts whether an image was generated by $F$ or was actually in $X$.

All four networks are trained in the usual GAN way until we‚Äôre left with powerful generators $G$ and $F$, which can perform the image-to-image translation task well enough to fool $D_Y$ and $D_X$ respectively.

This type of adversarial loss sounds like a good idea. But it isn‚Äôt enough. To further improve performance, CycleGAN uses another metric, cycle consistency loss.

Think about the properties of a good translator in general. One of them is that when you translate back and forth, you should get the same thing again.

CycleGAN implements this idea in a clever way. It forces the network to obey these constraints:

$$F(G(x)) \approx x, x \in X$$  
$$G(F(y)) \approx y, y \in Y$$

Visually, cycle consistency looks like this:

![Source](https://www.andrewszot.com/static/img/ml/voice_conversion/cyclegan.png)[Source](https://www.andrewszot.com/static/img/ml/voice_conversion/cyclegan.png)

The overall loss function is constructed in a way that penalizes the networks for not conforming to the above properties. I‚Äôm not going to write out the loss function here because that would spoil the way it comes together in the paper.

* * *

Ok, now before this becomes a Dragon-ball Z filler fest, let‚Äôs get back to our main quest of finding better GAN architectures.

## CoGAN: Coupled Generative Adversarial Networks

![](/assets/images/content/images/2019/06/cogan_face-2.png)Image from [paper](https://arxiv.org/pdf/1606.07536.pdf)

  * [Paper](https://arxiv.org/abs/1606.07536)
  * [Code](https://github.com/mingyuliutw/CoGAN)
  * Other great resources: [Blogpost](https://wiseodd.github.io/techblog/2017/02/18/coupled_gan/)

Do you know what‚Äôs better than one GAN? Two GANs!

CoGAN (which stands for ‚Äúcoupled generative adversarial network**s** ,‚Äù not to be confused with CGAN, which stands for conditional generative adversarial networks) does just that. It trains a ‚Äúcouple‚Äù of GANs rather than a single one.

Of course, GAN researchers just can‚Äôt stop making those cop and counterfeiter game theory analogies. So here‚Äôs the idea behind CoGAN, in the authors‚Äô own words:

> In the game, there are two teams, and each team has two players. The generative models form a team and work together for synthesizing a pair of images in two different domains for confusing the discriminative models. The discriminative models try to differentiate images drawn from the training data distribution in the respective domains from those drawn from the respective generative models. The collaboration between the players in the same team is established from the weight-sharing constraint.

Ok, so having a GAN multiplayer LAN party competition sounds nice and all, but how do you actually make it work?

It turns out it isn‚Äôt too complicated. Just make the networks use the **exact same** weights for some of the layers.

![Source](https://paper-attachments.dropbox.com/s_D85DDA7D01FD04AEE96825C4B90F1126BC7D080CA4F2947D4A5DEC07FAD6122C_1559833820903_CoGAN.png)[Source](https://arxiv.org/pdf/1606.07536.pdf)

In my not very humble opinion, the coolest thing about CoGAN isn‚Äôt the improved image generation quality or the fact you can train on multiple image domains.

It‚Äôs the fact that you get two images for the price of one-and-a-half.

Since we‚Äôre sharing some of the weights, a CoGAN would have fewer parameters (and hence would save more memory, compute, and storage) than two individual GANs.

It‚Äôs a subtle technique that‚Äôs ‚Äúout-of-fashion‚Äù and that some of the newer GANs that we see today don't use.

But one day, I reckon the underdog‚Äôs gonna make a comeback.

## ProGAN: Progressive growing of Generative Adversarial Networks

![](https://paper-attachments.dropbox.com/s_0A213C11DCFB5EF2AA80869C04C321E7C5012FF1A44CD8EA80EAEE3AC7E2E450_1552669280669_Screenshot+2019-03-15+at+18.00.57.png)Image from [paper](https://arxiv.org/pdf/1710.10196.pdf)

  * [Paper](https://arxiv.org/abs/1710.10196)
  * [Code](https://github.com/tkarras/progressive_growing_of_gans)
  * Other great resources: [Medium article](https://towardsdatascience.com/progan-how-nvidia-generated-images-of-unprecedented-quality-51c98ec2cbd2), [Demo video](https://www.youtube.com/watch?v=G06dEcZ-QTg)

There are many problems with training GANs. The most important of which is the training instability.

Sometimes, the loss of the GAN can oscillate, as the generator and discriminator undo the learning of the other. Other times, your loss may explode right after the networks converge, and the images start looking horrible.

ProGAN (which stands for the progressive growing of generative adversarial networks) is a technique that helps stabilize GAN training by incrementally increasing the resolution of the generated image.

The intuition here is that it‚Äôs easier to generate a 4x4 image than it is to generate a 1024x1024 image. Also, it‚Äôs easier to map a 16x16 image to a 32x32 image than it is to map a 2x2 image to a 32x32 image.

So ProGAN first trains a 4x4 generator and a 4x4 discriminator and adds layers that correspond to higher resolutions later in the training process. This animation sums up what I‚Äôm describing:

![Source](https://cdn-images-1.medium.com/max/1600/1*tUhgr3m54Qc80GU2BkaOiQ.gif)[Source](https://cdn-images-1.medium.com/max/1600/1*tUhgr3m54Qc80GU2BkaOiQ.gif)

## WGAN: Wasserstein Generative Adversarial Networks

![Source](https://paper-attachments.dropbox.com/s_D85DDA7D01FD04AEE96825C4B90F1126BC7D080CA4F2947D4A5DEC07FAD6122C_1559924418712_Screenshot+2019-06-07+at+9.50.06+PM.png)Image from [paper](https://arxiv.org/pdf/1701.07875.pdf)

  * [Paper](https://arxiv.org/abs/1701.07875v3)
  * [Code](https://github.com/eriklindernoren/Keras-GAN)
  * Other great resources: [DFL Curriculum](http://www.depthfirstlearning.com/2019/WassersteinGAN), [Blog post](https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html), [Another blog post](https://www.alexirpan.com/2017/02/22/wasserstein-gan.html), [Medium article](https://medium.com/@jonathan_hui/gan-wasserstein-gan-wgan-gp-6a1a2aa1b490)

This paper is perhaps the most theoretical/mathematical one on this list. The authors stuffed in a truckload of proofs, corollaries, another mathematical lingo into it. So if Integral probability metrics and Lipschitz continuity aren‚Äôt your things, I wouldn‚Äôt spend too much time on this one.

![](http://media.giphy.com/media/XyOrJljDNBEpa/giphy.gif)[Source](https://media.giphy.com/media/XyOrJljDNBEpa/giphy.gif)

In short, WGAN (the ‚ÄòW‚Äô stands for Wasserstein) proposes a new cost function that some nice properties that are all the rage for pure mathematicians and statisticians.

Here is the old version of the GAN minimax optimization game:

$$\min_{G}\max_{D}\mathbb{E}{x\sim p{\text{data}}(x)}[\log{D(x)}] + \mathbb{E}{z\sim p{\text{generated}}(z)}[1 - \log{D(G(z))}]$$

And here is the new one that WGAN uses:

$$\min_{G}\max_{D} \mathbb{E}{x\sim p{\text{data}}(x)}[D(x)] - \mathbb{E}{z \sim p{\text{generated}}(z)}[D(G(z))]$$

For the most part, that‚Äôs all you need to know to use WGAN in practice.

Just chuck out the old cost function, which approximates a statistical quantity called the Jensen-Shannon divergence, and slide in the new one, which approximates a statistical quantity called the 1-Wasserstein distance.

Here‚Äôs why that‚Äôs a big deal:

![Source](https://paper-attachments.dropbox.com/s_D85DDA7D01FD04AEE96825C4B90F1126BC7D080CA4F2947D4A5DEC07FAD6122C_1559924506531_Screenshot+2019-06-07+at+9.51.38+PM.png)Image from [paper](https://arxiv.org/pdf/1701.07875.pdf)

However, if you‚Äôre interested, what follows is a quick review of the mathematical details, which is the very reason that the WGAN paper was so highly acclaimed.

The original GAN paper showed that when the discriminator is optimal, the generator is updated in such a way to minimize the Jensen-Shannon divergence.

If you‚Äôre not familiar with it, the Jensen-Shannon divergence is a way of measuring how different two probability distributions are. The larger the JSD, the more ‚Äúdifferent‚Äù the two distributions are, and vice versa. You compute it like this:

$$\text{JSD}(P||Q) = \text{KL}(P || \frac{P+Q}{2}) + \text{KL}(Q || \frac{P+Q}{2})$$

$$KL(A||B) = \int_{-\infty}^{\infty}a(x)\log{\frac{a(x)}{b(x)}} dx$$

However, is minimizing the JSD the best thing to do?

The authors of the WGAN paper thought that it‚Äôs probably not. For a particular reason ‚Äî When the two distributions don‚Äôt overlap at all, you can show that the value of the JSD stays at a constant value of $2\log{2}$

A function that has has a constant value has a gradient equal to zero, and a zero gradient is bad because it means that the generator learns absolutely nothing.

The alternate distance metric proposed by the WGAN authors is the 1-Wasserstein distance, sometimes called the earth mover distance.

![Source](https://paper-attachments.dropbox.com/s_D85DDA7D01FD04AEE96825C4B90F1126BC7D080CA4F2947D4A5DEC07FAD6122C_1559923401643_Screenshot+2019-06-07+at+9.32.54+PM.png)Image from [paper](https://arxiv.org/pdf/1701.07875.pdf)

It gets the name ‚Äúearth mover distance‚Äù because of an analogy. Imagine that one of the two distributions is a pile of earth, and the other is a pit.

The earth mover distance measures the cost of transporting the pile of earth to the pit, assuming that you‚Äôre transporting the mud, sand, dirt, etc., as efficiently as possible. Here, ‚Äúcost‚Äù is considered to be $\text{distance between point} \times \text{amount of earth moved}$.

Concretely (no pun intended), the earth mover distance between two distributions can be written as:

$$\mathrm{EMD}(P_r, P_\theta) = \inf_{\gamma \in \Pi} , \sum\limits_{x,y} \Vert x - y \Vert \gamma (x,y) = \inf_{\gamma \in \Pi} \ \mathbb{E}_{(x,y) \sim \gamma} \Vert x - y \Vert$$

Where $inf$ is the infinimum (minimum), $x$ and $y$ are points on the two distributions, and $\gamma$ is the optimal transport plan.

Unfortunately, computing this is intractable. So instead, we compute something totally different:

$$\mathrm{EMD}(P_r, P_\theta) = \sup_{\lVert f \lVert_{L \leq 1}} \ \mathbb{E}{x \sim P_r} f(x) - \mathbb{E}{x \sim P_\theta} f(x)$$

The connection between these two equations certainly doesn‚Äôt seem evident at first, but through some fancy math called the [Kantorovich-Rubenstein duality](https://vincentherrmann.github.io/blog/wasserstein/) (try saying that three times fast), you can show that these formulas for the Wasserstein/earth mover distance are trying to calculate the same thing.

If you weren't able to follow along with some of the heavyweight math in the paper and blog posts that I‚Äôve linked to, don‚Äôt worry about it too much. Most of the work on WGAN is about providing a complex (read rigorous) justification for an admittedly simple idea.

## SAGAN: Self-Attention Generative Adversarial Networks

![Source](https://paper-attachments.dropbox.com/s_D85DDA7D01FD04AEE96825C4B90F1126BC7D080CA4F2947D4A5DEC07FAD6122C_1559905320679_Screenshot+2019-06-07+at+4.31.45+PM.png)Image from [paper](https://arxiv.org/pdf/1805.08318v1.pdf)

  * [Paper](https://arxiv.org/abs/1805.08318v1)
  * [Code](https://github.com/heykeetae/Self-Attention-GAN)
  * Other great resources: [Blog post](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html), [Medium article](https://towardsdatascience.com/not-just-another-gan-paper-sagan-96e649f01a6b)

Since GANs use transposed convolutions to ‚Äúscan‚Äù feature maps, they only have access to nearby information.

Using transposed convolutions alone is like painting a picture while only looking at the canvas region within a small radius of your paintbrush.

Even the greatest of artists, who manage to perfect the most exceptional and intricate details, need to take a step back and look at the big picture.

SAGAN (expanded as ‚Äúself-attention generative adversarial network‚Äù) uses the self-attention mechanism that has become extremely popular in recent years thanks to [the transformer architecture](https://floydhub.github.io/the-transformer-in-pytorch/).

Self-attention allows the generator to take a step back and look at the ‚Äúbig picture.‚Äù

## BigGAN

![Source](https://cdn-images-1.medium.com/max/2600/1*Yw2KxjmIkj8yqS-ykLCQCQ.png)[Source](https://cdn-images-1.medium.com/max/2600/1*Yw2KxjmIkj8yqS-ykLCQCQ.png)

  * [Paper](https://arxiv.org/abs/1809.11096v2)
  * [Code](https://github.com/huggingface/pytorch-pretrained-BigGAN)
  * Other great resources: [Two-minute papers video](https://www.youtube.com/watch?v=ZKQp28OqwNQ), [Gradient.pub post](https://thegradient.pub/bigganex-a-dive-into-the-latent-space-of-biggan/), [Medium article](https://medium.com/syncedreview/biggan-a-new-state-of-the-art-in-image-synthesis-cf2ec5694024)

After four long years, DeepMind decided to do with GANs what nobody else had attempted before. They used a mystic technique of deep learning so powerful that it made the most technically advanced models quiver in fear as it overtook everything on the state of the art leaderboards with plenty left to spare.

I present to you BigGAN, the GAN that does absolutely nothing (but is run a bunch of TPU clusters, yet somehow deserves to be on this list).

![](http://media.giphy.com/media/26BRGxMMN3Pn1MMdG/giphy.gif)[Source](https://media.giphy.com/media/26BRGxMMN3Pn1MMdG/giphy.gif)

Jokes apart, the DeepMind team did accomplish a lot with BigGAN. Apart from getting all the eyeballs with the realistic imagery, BigGAN showed us some very detailed results of training GANs at large scales.

The team behind BigGAN introduced a variety of techniques to combat the instability of training GANs on huge batch sizes across many machines.

To start with, DeepMind used SAGAN as a baseline and tacked on a feature called [spectral normalization](https://arxiv.org/abs/1802.05957).

From there, they scaled the batch size by 50%, width (number of channels) by 20%. Initially, increasing the number of layers didn‚Äôt seem to help.

After a few other single digit percentage improvements, the authors use a ‚Äútruncation trick‚Äù to improve the quality of the _sampled_ image.

While BigGAN samples its latent vector from $z ~ \mathcal{N}(0, I)$ during training, it resamples the latent vector if it falls outside a given range when generating images.

The range is a hyperparameter, represented by $\psi$. A smaller $\psi$ tightens the range, which increases sample fidelity at the cost of variety.

So what do all of these intricate tuning effort lead to? Well, some call it dogball:

![](/assets/images/content/images/2019/06/dogball.jpg)[Source](https://regmedia.co.uk/2018/10/01/dogball.jpg)

BigGAN also shows that training GANs at a larger scale can have its own set of problems. Notably, training seems to scale well by increasing parameters like batch size and width, but for some reason, collapses at the very end.

If analyzing singular values to understand this instability sounds interesting to you, check out the paper, because you'll find a lot it there.

Finally, the authors also train BigGAN on a new dataset called JFT-300, which is an ImageNet-like dataset which has, you guessed it, 300 million images. They showed that BigGAN perform better on this dataset, suggesting that more massive datasets might be the way to go for GANs

After the first version of the paper was released, the authors revisited BigGAN a few months later. Remember how I said that increasing the number of layers didn‚Äôt work? Turns out that was due to a poor architectural choice.

Instead of just cramming more layers onto the model, the team experimented and found that using a ResNet bottleneck is the way to go.

With all of the above tweaks, scaling, and careful experimentation, the top of the line BigGAN completely obliterates the previous state of the art Inception score of 52.52 with a whopping score of 152.8.

If that isn‚Äôt progress, I don‚Äôt know what is.

## StyleGAN: Style-based Generative Adversarial Networks

![](https://paper-attachments.dropbox.com/s_0A213C11DCFB5EF2AA80869C04C321E7C5012FF1A44CD8EA80EAEE3AC7E2E450_1552669593074_Screenshot+2019-03-15+at+18.06.21.png)Image from [paper](https://arxiv.org/abs/1812.04948)

  * [Paper](https://arxiv.org/abs/1812.04948)
  * [Code](https://github.com/NVlabs/stylegan)
  * Other great resources: [thispersondoesnotexist](https://thispersondoesnotexist.com/), [Blog post](https://blog.nanonets.com/stylegan-got/), [Another blog post](https://www.gwern.net/Faces), [Technical summary article](https://www.lyrn.ai/2018/12/26/a-style-based-generator-architecture-for-generative-adversarial-networks/)

StyleGAN (short for well, style generative adversarial network?) is a development from Nvidia research that is mostly orthogonal to the more traditional GAN research, which focuses on loss functions, stabilization, architectures, etc.

Having a world-class face generator that can fool most humans on planet earth is pointless if you want to generate images of cars.

So instead of focusing on creating more realistic images, StyleGAN improves a GANs capability to have fine control over the image that‚Äôs generated.

As I mentioned, StyleGAN doesn‚Äôt develop on architectures and loss functions. Instead, is a suite of techniques that can be used with any GAN to allow you to do all sorts of cool things like mix images, vary details at multiple levels, and perform a more advanced version of style transfer.

In other words, StyleGAN is like a photoshop plugin, while most GAN developments are a new version of photoshop.

To achieve this level of image style control, StyleGAN employs existing techniques like Adaptive instance normalization, a latent vector mapping network, and a constant learned input.

It‚Äôs hard to describe StyleGAN any further without getting into the details, so if you‚Äôre interested, check out my article where I show [how to generate Game of Thrones characters using StyleGAN](https://blog.nanonets.com/stylegan-got/). I have a detailed explanation of all the techniques, with a **lot** of cool results along the way.

## Conclusion

Wow, you made it to the end. Congratulation! You're all caught up on the latest breakthroughs in the highly academic domain of creating fake profile pics.

But before you slump onto your couch and begin your endless scroll of your Twitter feed, take a moment to look at how far you've come:

![](/assets/images/content/images/2019/06/Screenshot-2019-06-06-at-7.39.35-PM-1.png)What's next?! Unexplored regions!

After climbing the mountains of ProGAN and StyleGAN, and voyaging across the computational sea to arrive at the vast fields of BigGAN, it's easy to get lost around these parts.

But zoom in. Take a closer look. Do you see that show-shaped green patch of land? The red delta in the north?

Those are the regions unexplored. The breakthroughs waiting to be made. They can all be yours if you choose to take the leap of faith.

Farewell, my friend, there are yet more seas to sail.

## Epilogue: Some Interesting Modern Research

Thanks for reading this article. By now, If you've followed all the resources I shared, you should have a solid understanding of some of the most important breakthroughs in GAN technology.

But undoubtedly, there will be more. Keeping up with research will be tough, but it's not impossible. I'd recommend that you try to stick to newer papers since they are likely to produce the best results in your projects.

To get you started, here are a few bleeding-edge (as of May 2019) research projects:

  * You've probably heard of DeOldify by now. If not, jump [here](https://floydhub.github.io/colorizing-and-restoring-old-images-with-deep-learning/)! But there's been a recent update to it that introduces a new training technique called NoGAN. You can check out the details in their [blog](https://www.fast.ai/2019/05/03/decrappify/) and [code](https://github.com/jantic/DeOldify).
  * If you don't have Google-level amounts of data, it can be challenging to replicate the BigGAN results from scratch. There's now an [ICML 2019 paper](https://arxiv.org/pdf/1903.02271.pdf) that proposes to train a BigGAN quality model with fewer labels.
  * Of course, GANs aren't the only deep learning based technique for image generation. Recently, OpenAI unveiled a completely new model called the sparse transformer that leverages the transformer architecture for image generation. As usual, they released a [paper](https://arxiv.org/abs/1904.10509), [blog](https://openai.com/blog/sparse-transformer/), and [code](https://github.com/openai/sparse_attention).
  * Oh, and this isn't new research or anything, but you should listen to the origin story of GANs:

  * Nvidia has this really cool project called GauGAN which can turn your child's scribblings into photorealistic masterpieces. This is truly something that you need to experience to understand. So [play around with the demo](https://www.nvidia.com/en-us/research/ai-playground/?ncid=so-twi-nz-92489) first, then read their [blog](https://blogs.nvidia.com/blog/2019/03/18/gaugan-photorealistic-landscapes-nvidia-research/) and [paper](https://arxiv.org/abs/1903.07291).
  * Have you ever wondered how to "debug" a GAN? There's now [[an ICLR 2019 paper](https://openreview.net/pdf?id=Syx_Ss05tm)](https://openreview.net/pdf?id=Syx_Ss05tm) that proposes a promising solution.
  * Despite how cool I made GANs look, there's plenty of work to be done. There's [an excellent distill article that summarizes some of the unsolved problems](https://distill.pub/2019/gan-open-problems/).
  * It looks like someone has found another type of real-world application for GANs.

> Apparently, GANs are used to create fake profile pictures on LinkedIn for international industrial espionage. <https://t.co/IFYJwb30cw>
> 
> -- Yann LeCun (@ylecun) [June 13, 2019](https://twitter.com/ylecun/status/1139027051653074949?ref_src=twsrc%5Etfw)

* * *

### Do you model for living? üë©‚Äçüíª ü§ñ Be part of a ML/DL user research study and get a cool AI t-shirt every month üí•

  
We are looking for _full-time data scientists_ for a ML/DL user study. You'll be participating in a calibrated user research experiment for 45 minutes. The study will be done over a video call. We've got plenty of funny tees that you can show-off to your teammates. We'll ship you a different one every month for a year!

Click [here](https://typings.typeform.com/to/zpYrlW?utm_source=blog&utm_medium=bottom_text_gans&utm_campaign=full_time_ds_user_study) to learn more.

* * *

#### ******FloydHub Call for AI writers******

Want to write amazing articles like Ajay and play your role in the long road to Artificial General Intelligence? [We are looking for passionate writers](https://floydhub.github.io/write-for-floydhub/?utm_source=floydhub&utm_medium=banner&utm_campaign=call_for_writers_2019), to build the world's best blog for practical applications of groundbreaking A.I. techniques. FloydHub has a large reach within the AI community and with your help, we can inspire the next wave of AI. [Apply now](https://goo.gl/forms/PbOw0VmUnOfO1Lxp1) and join the crew!

* * *

****About Ajay Uppili Arasanipalai****

Ajay is a deep learning enthusiast and student at the University of Illinois at Urbana-Champaign. Ajay is a [FloydHub AI Writer](https://floydhub.github.io/write-for-floydhub/) (you can read his previous FloydHub article [here](https://floydhub.github.io/gpt2/)). He writes technical articles for blogs like FloydHub, Freecodecamp, HackerNoon, and his own blog, [Elliptigon](https://ajay.elliptigon.com/). You can connect with Ajay via [LinkedIn](https://www.linkedin.com/in/ajay-arasanipalai-712911164/), [Twitter](https://twitter.com/iyajainfinity), [Medium](https://medium.com/@ajayuppili), [Reddit](https://www.reddit.com/user/iyaja), and [GitHub](https://github.com/iyaja).